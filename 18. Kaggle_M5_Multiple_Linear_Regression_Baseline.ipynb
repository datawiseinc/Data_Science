{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle: M5_Accuracy Competition _ Predictive Model Baseline\n",
    "\n",
    "Competition Link: https://www.kaggle.com/c/m5-forecasting-accuracy\n",
    "\n",
    "After preprocessing the competition files using the preprocessing pipeline notebook, locate the training, test, validation files and run all.\n",
    "\n",
    "Training file has approximately 65 million rows, and model uses 59 columns for training.\n",
    "\n",
    "Features are standard features, and besides encoding the categorical variables, no features are engineered. \n",
    "\n",
    "MSE = 0.7\n",
    "\n",
    "For prediction:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries, adjust the setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "x=time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set precision to 3 for easier reading\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "#set the number of columns and rows visible with pandas dataframes\n",
    "pd.set_option('max_column', None)\n",
    "pd.set_option('max_rows', 999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Save Schema and Training columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema of the final processed file\n",
    "schema = {'date':'O','id':'O', 'item_id':'O', 'dept_id':'O', 'cat_id':'O',\n",
    " 'store_id':'O', 'state_id':'O', 'd':'O', 'weekday':'O', 'event_name_1':'O',\n",
    " 'event_type_1':'O', 'event_name_2':'O', 'event_type_2':'O', 'day':'int16',\n",
    " 'wm_yr_wk':'int16', 'year':'int16', 'month':'int8', 'wday':'int8', 'snap_CA':'int8',\n",
    " 'snap_TX':'int8', 'snap_WI':'int8', 'Eid al-Fitr':'int8', 'Pesach End':'int8',\n",
    " \"Father's day\":'int8', 'ValentinesDay':'int8', 'NBAFinalsStart':'int8', 'NewYear':'int8',\n",
    " 'Chanukah End':'int8', 'StPatricksDay':'int8', 'LentWeek2':'int8', 'Cinco De Mayo':'int8',\n",
    " 'Christmas':'int8', \"Mother's day\":'int8', 'ColumbusDay':'int8', 'VeteransDay':'int8',\n",
    " 'IndependenceDay':'int8', 'SuperBowl':'int8', 'Easter':'int8', 'Halloween':'int8',\n",
    " 'MartinLutherKingDay':'int8', 'OrthodoxChristmas':'int8', 'Thanksgiving':'int8', 'OrthodoxEaster':'int8',\n",
    " 'EidAlAdha':'int8', 'NBAFinalsEnd':'int8', 'PresidentsDay':'int8', 'LentStart':'int8',\n",
    " 'Ramadan starts':'int8', 'Purim End':'int8', 'LaborDay':'int8', 'MemorialDay':'int8',\n",
    " 'Cultural':'int8', 'National':'int8', 'Religious':'int8', 'Sporting':'int8', 'FOODS_2':'int8',\n",
    " 'FOODS_3':'int8', 'HOBBIES_1':'int8', 'HOBBIES_2':'int8', 'HOUSEHOLD_1':'int8',\n",
    " 'HOUSEHOLD_2':'int8', 'HOBBIES':'int8', 'HOUSEHOLD':'int8', 'TX':'int8', 'WI':'int8',\n",
    " 'CA_2':'int8', 'CA_3':'int8', 'CA_4':'int8', 'TX_1':'int8', 'TX_2':'int8',\n",
    " 'TX_3':'int8', 'WI_1':'int8', 'WI_2':'int8', 'WI_3':'int8', 'sell_price':'float32',\n",
    " 'quantity_sold':'int32'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cols = ['id','item_id','day', 'month','wday','CA_2', 'CA_3', 'CA_4', \n",
    "                 'Chanukah End', 'Christmas', 'Cinco De Mayo', 'ColumbusDay', \n",
    "                 'Cultural', 'Easter', 'Eid al-Fitr', 'EidAlAdha', 'FOODS_2', \n",
    "                 'FOODS_3', \"Father's day\", 'HOBBIES', 'HOBBIES_1', 'HOBBIES_2',\n",
    "                 'HOUSEHOLD', 'HOUSEHOLD_1', 'HOUSEHOLD_2', 'Halloween', 'IndependenceDay',\n",
    "                 'LaborDay', 'LentStart', 'LentWeek2', 'MartinLutherKingDay', 'MemorialDay',\n",
    "                 \"Mother's day\", 'NBAFinalsEnd', 'NBAFinalsStart', 'National', \n",
    "                 'NewYear', 'OrthodoxChristmas', 'OrthodoxEaster', 'Pesach End', \n",
    "                 'PresidentsDay', 'Purim End', 'Ramadan starts', 'Religious', 'Sporting',\n",
    "                 'StPatricksDay', 'SuperBowl', 'TX', 'TX_1', 'TX_2', 'TX_3', 'Thanksgiving',\n",
    "                 'ValentinesDay', 'VeteransDay', 'WI', 'WI_1', 'WI_2', 'WI_3', \n",
    "                 'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'quantity_sold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### identify the input and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns to use for training\n",
    "input_cols = ['month', 'wday', 'snap_CA', 'snap_TX', 'snap_WI',\n",
    "       'Eid al-Fitr', 'Pesach End', 'Father\\'s day', 'ValentinesDay',\n",
    "       'NBAFinalsStart', 'NewYear', 'Chanukah End', 'StPatricksDay',\n",
    "       'LentWeek2', 'Cinco De Mayo', 'Christmas', 'Mother\\'s day',\n",
    "       'ColumbusDay', 'VeteransDay', 'IndependenceDay', 'SuperBowl', 'Easter',\n",
    "       'Halloween', 'MartinLutherKingDay', 'OrthodoxChristmas', 'Thanksgiving',\n",
    "       'OrthodoxEaster', 'EidAlAdha', 'NBAFinalsEnd', 'PresidentsDay',\n",
    "       'LentStart', 'Ramadan starts', 'Purim End', 'LaborDay', 'MemorialDay',\n",
    "       'Cultural', 'National', 'Religious', 'Sporting', 'FOODS_2', 'FOODS_3',\n",
    "       'HOBBIES_1', 'HOBBIES_2', 'HOUSEHOLD_1', 'HOUSEHOLD_2', 'HOBBIES',\n",
    "       'HOUSEHOLD', 'TX', 'WI', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3',\n",
    "       'WI_1', 'WI_2', 'WI_3', 'sell_price']\n",
    "\n",
    "target_col = 'quantity_sold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the submission file column names to a variable\n",
    "submission_columns = ['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10',\n",
    "       'F11', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20',\n",
    "       'F21', 'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_path = '../../../../Documents/KAGGLE/WALMART/m5_training_df.csv'\n",
    "\n",
    "validation_file_path = '../../../../Documents/KAGGLE/WALMART/m5_validation_df.csv'\n",
    "\n",
    "testing_file_path = '../../../../Documents/KAGGLE/WALMART/m5_testing_df.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the training file\n",
    "training_df = pd.read_csv(training_file_path, index_col=0, dtype=schema, usecols=training_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the validation file\n",
    "validation_df = pd.read_csv(validation_file_path, usecols=training_cols, dtype=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the testing file\n",
    "testing_df = pd.read_csv(testing_file_path, usecols=training_cols, dtype=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the file locations and names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = input('Enter a name for the results file?: ')\n",
    "file_path = '../../../../Documents/KAGGLE/WALMART/'\n",
    "file_name = file_path+file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframes to save the results into a new dataframe\n",
    "\n",
    "weight_dict = {}  #ths will store the best weights for each item\n",
    "\n",
    "#make a list of the unique item_ids\n",
    "items = training_df.item_id.unique()\n",
    "\n",
    "for item in items:\n",
    "    \n",
    "    \n",
    "    temp_df = training_df[training_df['item_id'] == item]\n",
    "\n",
    "    X = temp_df[input_cols]   #filter the dataset on the item_id\n",
    "\n",
    "    target = np.array(temp_df[target_col]).reshape(-1,1)   #labels\n",
    "\n",
    "\n",
    "\n",
    "    #initialize coefficients and the bias\n",
    "    np.random.seed(1)\n",
    "    W = np.random.rand(59, 1)/10\n",
    "    B = np.random.random(size=(1))/10\n",
    "    N = np.random.rand(X.shape[0],1)/10\n",
    "\n",
    "    #Set parameters\n",
    "    epochs = 30\n",
    "    early_stop_threshold = 3\n",
    "    learning_rate = 0.001\n",
    "    num_observations = X.shape[0]\n",
    "\n",
    "    #initialize the loss and and a loss list to help with early stopping\n",
    "    losses = [0]\n",
    "    loss_score = 0\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        Y =  X.dot(W) +  B + N  #calculate the targets\n",
    "\n",
    "        deltas = target - Y   #Find the error\n",
    "\n",
    "        loss = deltas.pow(2).mean()   #Loss function\n",
    "\n",
    "        \n",
    "\n",
    "        #set up an early stopping mechanism\n",
    "        #add the loss to the losses list\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if loss[0] > loss_score:\n",
    "\n",
    "            loss_score += 1   #keep track of the change in loss function\n",
    "\n",
    "\n",
    "        print(f'Epoch: {epoch}  Loss: {loss[0]:.3f}')\n",
    "\n",
    "\n",
    "        if loss_score > early_stop_threshold or epoch == (epochs-1):   #If the loss is increasing 10 times or finished epochs stop learning\n",
    "\n",
    "            weight_dict[item] = (W,B)\n",
    "            \n",
    "            \n",
    "            #prepare a submission file with the validation file\n",
    "            \n",
    "            result = validation_df[validation_df['item_id'] == item][['id', 'item_id', 'day', 'quantity_sold']]   #create a new dataframe to save predictions\n",
    "\n",
    "            result['predictions_test'] =  validation_df[validation_df.item_id==item][input_cols].dot(weight_dict[item][0])+weight_dict[item][1]#save the predictions\n",
    "\n",
    "            result.to_csv(file_name + '_validation.csv', mode='a', header=False)\n",
    "            \n",
    "            \n",
    "            #prepare a submission file with the ground truth file\n",
    "            result = testing_df[testing_df['item_id'] == item][['id', 'item_id', 'day', 'quantity_sold']]   #create a new dataframe to save predictions\n",
    "\n",
    "            result['predictions_test'] =  testing_df[testing_df.item_id==item][input_cols].dot(weight_dict[item][0])+weight_dict[item][1]#save the predictions\n",
    "\n",
    "            result.to_csv(file_name + '_test.csv', mode='a', header=False)\n",
    "          \n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "        #scale the deltas\n",
    "        deltas_scaled = deltas / num_observations\n",
    "\n",
    "        #Optimize\n",
    "        #update the weights biases\n",
    "        W = W -learning_rate * np.dot(X.T, deltas_scaled)\n",
    "        B = B - learning_rate * np.sum(deltas_scaled)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(file_name+'_test.csv', index_col=0, header=None)\n",
    "submission.columns = ['id', 'item_id', 'day', 'quantity_sold', 'predictions']\n",
    "submission_file = submission.drop(['item_id','quantity_sold'], axis=1)\n",
    "submission_file = submission_file.pivot_table(index=['id'], columns='day', values='predictions')\n",
    "submission_file.columns = submission_columns\n",
    "for col in submission_file:\n",
    "    submission_file [col] = submission_file[col].apply(lambda x: x if x>0 else 0)\n",
    "    \n",
    "submission_file.to_csv(file_path+'m5_test_submission_ML_GD_LR-001_EStop-3_RSeed-1_Loss-MSE_Epoch-30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(file_name+'_validation.csv', index_col=0, header=None)\n",
    "submission.columns = ['id', 'item_id', 'day', 'quantity_sold', 'predictions']\n",
    "submission_file = submission.drop(['item_id','quantity_sold'], axis=1)\n",
    "submission_file = submission_file.pivot_table(index=['id'], columns='day', values='predictions')\n",
    "submission_file.columns = submission_columns\n",
    "for col in submission_file:\n",
    "    submission_file [col] = submission_file[col].apply(lambda x: x if x>0 else 0)\n",
    "    \n",
    "submission_file.to_csv(file_path+'m5_validation_submission_ML_GD_LR-001_EStop-3_RSeed-1_Loss-MSE_Epoch-30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'It took {(y-x)/60: .1f} minutes to complete the run')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "434.8px",
    "left": "870.6px",
    "right": "20px",
    "top": "57px",
    "width": "640.2px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
